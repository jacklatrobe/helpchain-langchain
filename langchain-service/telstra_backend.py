## Helpchain-Langchain: Microservices components for the langchain component of the GPT solution
## File: langchain-service/telstra-backend.py

import os
import io
import json
import yaml
import requests
import validators
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from pypdf import PdfReader
from langchain.llms import OpenAI
from langchain.utilities import WikipediaAPIWrapper
from langchain.utilities import OpenWeatherMapAPIWrapper
from langchain.utilities import SerpAPIWrapper
from langchain.agents import Tool
from langchain.agents import initialize_agent
from langchain.agents import AgentType

# I've had hella trouble with env files, so being strict on these checks
if os.path.isfile("helpchain.env"):
    load_dotenv("helpchain.env")
else:
    raise FileNotFoundError("helpchain.env file not located")


# intelligent_response - entry point and wrapper function
def handle_telstra_query(query):
    if os.environ.get("OPENAI_API_KEY") is None:
        return "OpenAI API key error"
    if os.environ.get("OPENWEATHERMAP_API_KEY") is None:
        return "OpenWeatherMap API key error"
    if os.environ.get("SERPER_API_KEY") is None:
        return "Serper API key error"

    prompt = "Provide an accurate and well-researched answer to the following question, include any relevant links, and answer 'I don't know' if you are unable to find any relevant research to back up your answer:\n{}".format(query)
    token_llm = OpenAI()
    prompt_tokens = int(400 + (1.25 * token_llm.get_num_tokens(prompt)))
    if(prompt_tokens > 1000):
        return "Error generating response - prompt was too long"
    control_llm = OpenAI(temperature=0, max_tokens=(3000-prompt_tokens))
    tools = [
        Tool(
            name="Intermediate Answer",
            func=researcher,
            description="this tool looks up accurate answers to questions"
        )
    ]   

    agent = initialize_agent(tools, control_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
    result = agent.run(prompt)

    return result
  
# researcher - Agent to handle all external calls and research
def researcher(query):
    wikipedia = WikipediaAPIWrapper(top_k_results=1)
    weather = OpenWeatherMapAPIWrapper()
    prompt="Write an answer to the following question about Telstra, including links to where you found the information. Never use links generated by the LLM, only URLs you find through a Google Search. The research question is: {}".format(query)
    token_llm = OpenAI()
    prompt_tokens = int(400 + (1.25 * token_llm.get_num_tokens(prompt)))
    if(prompt_tokens > 1000):
        return "Error generating response - prompt was too long, please rephrase your query as a research question"
    control_llm = OpenAI(temperature=0.1, max_tokens=1000)
    tools = [
        Tool(
            name="Google Search",
            func=googler,
            description="A low-cost Google Search API. Useful for when you need to answer questions. Input should be a search query."
        ),
        Tool(
            name="Wikipedia Search",
            func=wikipedia.run,
            description="Useful for searching for encyclopedia articles about people, places, concepts and general information. This tool takes key words or search terms as an input"
        ),
        Tool(
            name="Weather Lookup",
            func=weather.run,
            description="Useful for when you need to lookup the current weather in a specific location. The input for this tool must be in the format 'CITY, COUNTRY'"
        ),
        Tool(
            name="Read a webpage",
            func=read_webpage,
            description="Useful for clicking on a link or reading a single web page where you know the link. The input for this tool must be a valid URL"
        ),
        Tool(
            name="Read a PDF",
            func=read_pdf,
            description="Useful for opening and reading PDF documents where you know the link. The input for this tool must be a valid PDF URL"
        )
    ]
    agent = initialize_agent(tools, control_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
    response = agent.run(prompt)
    return response

# googler - Returns google search results from the Serper API
def googler(query):
    url = "https://google.serper.dev/search"
    payload = json.dumps({
        "q": 'site:telstra.com.au {}'.format(query),
        "gl": "au",
        "hl": "en",
        "k": 3,
    })
    print(payload)
    headers = {
        'X-API-KEY': os.environ.get("SERPER_API_KEY"),
        'Content-Type': 'application/json'
    }
    response = requests.request("POST", url, headers=headers, data=payload)
    search_results = response.json()
    print(search_results)
    snippets = {}
    if search_results.get("knowledgeGraph"):
        kg = search_results.get("knowledgeGraph", {})
        snippets.update({"title": kg.get("title")})
        entity_type = kg.get("type")
        if entity_type:
            snippets.update({"entity_type" : entity_type})
        description = kg.get("description")
        if description:
            snippets.update({"description": description})
        for attribute, value in kg.get("attributes", {}).items():
            snippets.update({"{}".format(attribute) : "{}".format(value)})

    links = {}
    for result in search_results.get("organic"):
        links.update({"link_title" : result.get("title"),
                        "link_url" : result.get("link")})
    if len(links) > 0:
        snippets.update({"links": links})

    if len(snippets) == 0:
        return "No good Google Search Result was found"
    else:
        yaml_response = yaml.dump(snippets)
        print(yaml_response)
        return yaml_response
    
# read_the_docs - Opens a single webpage, article or document and gets the LLM to convert and summarise it
def read_webpage(query):
    if not validators.url(query):
        return "This is not a valid URL - unable to look up webpage, article or document"
    if ".pdf" in query:
        return "This tool is unable to read PDF documents - please provide a link to a standard HTML web page"
    try:
        f = requests.get(query)
        html = BeautifulSoup(f.content, "html.parser")
        body = " ".join(html.body.text.split())
        texts = split_str(body, 5000)
    except ValueError as ex:
        return "Error loading and parsing data from web page"

    llm = OpenAI(temperature=0, max_tokens=2000)
    responses = []
    for text in texts:
        responses.append(llm("Clean up and summarise this text copied from a web page: {}".format(text)))
    response = "\n".join(responses)
    token_llm = OpenAI()
    prompt_tokens = int(400 + (1.25 * token_llm.get_num_tokens(response)))
    if(prompt_tokens < 500):
        return response
    elif(prompt_tokens < 2000):
        return llm("Join and summarise these blocks of text: {}".format(response))
    else:
        return "Page was too long to load and summarise - try a different URL"
    
def split_str(seq, chunk, skip_tail=False):
    lst = []
    if chunk <= len(seq):
        lst.extend([seq[:chunk]])
        lst.extend(split_str(seq[chunk:], chunk, skip_tail))
    elif not skip_tail and seq:
        lst.extend([seq])
    return lst

def read_pdf(url):
    if not validators.url(url):
        return "This is not a valid URL - unable to look up webpage, article or document"
    if ".pdf" not in url:
        return "This tool can only read PDF documents - please provide a link to a PDF file"
    f = requests.get(url)
    bytestream = io.BytesIO(f.content)
    pdf = PdfReader(bytestream)
    llm = OpenAI(temperature=0)
    text_pages = []
    page_count = 1
    for page in pdf.pages:
        print("\nProcessing PDF page: {}".format(page_count))
        text = page.extract_text()
        page_content = llm("Neatly reformat this page of text copied from a PDF document: {}".format(text))
        text_pages.append("  page_{}_content: {}".format(page_count,page_content))
        page_count += 1
    text = "\n  ".join(text_pages)
    return "pdf_url: {}\n\npdf_content:\n{}".format(url,text)